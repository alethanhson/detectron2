import os
from detectron2.data.datasets import register_coco_instances
from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader
from detectron2.config import get_cfg
from detectron2.modeling import build_model
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.evaluation import (
    COCOEvaluator,
    inference_on_dataset,
    DatasetEvaluators,
    DatasetEvaluator
)

# --- ƒê·ªãnh nghƒ©a Evaluator t√πy ch·ªânh: Counter ---
class Counter(DatasetEvaluator):
    def reset(self):
        self.count = 0

    def process(self, inputs, outputs):
        # Gi·∫£ s·ª≠ output ch·ª©a key "instances"
        for output in outputs:
            if "instances" in output:
                self.count += len(output["instances"])

    def evaluate(self):
        return {"detected_instances_count": self.count}

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ---
MODEL_OLD = "models/model_final_detect.pth"
MODEL_NEW = "models/mask_rcnn_R_101_FPN_3x/time_2/model_0069999.pth"
IMG_DIR = "annotations/data_test/data_coco_summary_test_1"
ANN_FILE = "annotations/data_test/data_coco_summary_test_1/_annotations.json"
DATASET_NAME = "eval_dataset"

# --- Ki·ªÉm tra n·∫øu checkpoint kh√¥ng t·ªìn t·∫°i ---
if not os.path.exists(MODEL_OLD) or not os.path.exists(MODEL_NEW):
    raise FileNotFoundError("‚ùå Model checkpoint kh√¥ng t·ªìn t·∫°i.")

# --- ƒêƒÉng k√Ω dataset test theo ƒë·ªãnh d·∫°ng COCO ---
register_coco_instances(DATASET_NAME, {}, ANN_FILE, IMG_DIR)
dataset_dicts = DatasetCatalog.get(DATASET_NAME)
metadata = MetadataCatalog.get(DATASET_NAME)
print(f"üìå Dataset '{DATASET_NAME}' ƒë√£ ƒëƒÉng k√Ω v·ªõi {len(dataset_dicts)} ·∫£nh.")

# --- C·∫•u h√¨nh model ---
cfg = get_cfg()
# B·∫°n c√≥ th·ªÉ merge config t·ª´ file c·∫•u h√¨nh c·ªßa Detectron2 (v√≠ d·ª•: t·ª´ model zoo)
# cfg.merge_from_file("path/to/config.yaml")
cfg.MODEL.WEIGHTS = MODEL_NEW
cfg.DATASETS.TEST = (DATASET_NAME,)
# N·∫øu c·∫ßn, c·∫≠p nh·∫≠t th√™m c√°c tham s·ªë nh∆∞ k√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o
cfg.INPUT.MIN_SIZE_TEST = 256
cfg.INPUT.MAX_SIZE_TEST = 256
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4

# --- X√¢y d·ª±ng model v√† load checkpoint ---
model = build_model(cfg)
cfg.OUTPUT_DIR = "./output"
os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
checkpointer = DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR)
checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=False)
model.eval()

# --- X√¢y d·ª±ng data loader cho dataset test ---
data_loader = build_detection_test_loader(cfg, DATASET_NAME)

# --- Kh·ªüi t·∫°o evaluator: s·ª≠ d·ª•ng COCOEvaluator v√† custom Counter evaluator ---
output_dir = os.path.join(cfg.OUTPUT_DIR, "inference", DATASET_NAME)
os.makedirs(output_dir, exist_ok=True)
coco_evaluator = COCOEvaluator(DATASET_NAME, cfg, False, output_dir=output_dir)
counter_evaluator = Counter()

# S·ª≠ d·ª•ng DatasetEvaluators ƒë·ªÉ g·ªôp c√°c evaluator l·∫°i v·ªõi nhau
combined_evaluator = DatasetEvaluators([coco_evaluator, counter_evaluator])

# --- Ch·∫°y inference v√† ƒë√°nh gi√° ---
print("üöÄ B·∫Øt ƒë·∫ßu ƒë√°nh gi√° model tr√™n dataset test...")
results = inference_on_dataset(model, data_loader, combined_evaluator)
print("üîç K·∫øt qu·∫£ ƒë√°nh gi√°:")
for k, v in results.items():
    print(f"  {k}: {v}")
